{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMDnwqb7ZHCKpsX4b/gyDq3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OSi--K_1TTSJ","executionInfo":{"status":"ok","timestamp":1683876095886,"user_tz":-180,"elapsed":9255,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"outputs":[],"source":["import torch, torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import time\n","import numpy as np\n","import os\n","from matplotlib import pyplot as plt\n","import cv2\n","import os\n","import random\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"code","source":["#Зафиксируем seed для воспроизводимости\n","def seed_everything(seed):\n","    random.seed(seed) # фиксируем генератор случайных чисел\n","    os.environ['PYTHONHASHSEED'] = str(seed) # фиксируем заполнения хешей\n","    np.random.seed(seed) # фиксируем генератор случайных чисел numpy\n","    torch.manual_seed(seed) # фиксируем генератор случайных чисел pytorch"],"metadata":{"id":"JDTo_Bb_Tsv6","executionInfo":{"status":"ok","timestamp":1683876131137,"user_tz":-180,"elapsed":1316,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["seed_everything(42)"],"metadata":{"id":"VmL6g9oqTvmS","executionInfo":{"status":"ok","timestamp":1683876133908,"user_tz":-180,"elapsed":2,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Applying Transforms to the Data\n","image_transforms = { \n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","}"],"metadata":{"id":"zWS_UEbv2hNf","executionInfo":{"status":"ok","timestamp":1683877930393,"user_tz":-180,"elapsed":391,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Load the Data\n","\n","# Set train and valid directory paths\n","\n","dataset = 'cifar10'\n","\n","train_directory = os.path.join(dataset, 'train')\n","valid_directory = os.path.join(dataset, 'valid')\n","\n","# Batch size\n","bs = 32\n","\n","# Number of classes\n","num_classes = 10\n","\n","# Load Data from folders\n","data = {\n","    'train': torchvision.datasets.CIFAR10(root=\"./cifar\", train=True, download=True, transform=image_transforms['train']),\n","    'valid': torchvision.datasets.CIFAR10(root=\"./cifar\", train=False, download=True, transform=image_transforms['valid']),\n","}\n","\n","#Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n","idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n","print(idx_to_class)\n","\n","# Size of Data, to be used for calculating Average Loss and Accuracy\n","train_data_size = len(data['train'])\n","valid_data_size = len(data['valid'])\n","\n","# Create iterators for the Data loaded using DataLoader module\n","train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=True)\n","valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGwHiOkV2hX-","executionInfo":{"status":"ok","timestamp":1683877934437,"user_tz":-180,"elapsed":1984,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"cf4784c0-c477-444e-afb0-b7e2f87063f0"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","{0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"as-GbcjTT0x6","executionInfo":{"status":"ok","timestamp":1683876285886,"user_tz":-180,"elapsed":519,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f7146484-fe22-428c-d166-35f394440ab5"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["MobileNet 3й версии, Small."],"metadata":{"id":"STDWhtYU7L-i"}},{"cell_type":"code","source":["mobilenet=models.mobilenetv3.mobilenet_v3_small(weights='DEFAULT')\n","mobilenet=mobilenet.to(device)"],"metadata":{"id":"KdvQZIk1UITr","executionInfo":{"status":"ok","timestamp":1683876293738,"user_tz":-180,"elapsed":5648,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Freeze model parameters\n","for param in mobilenet.parameters():\n","    param.requires_grad = False"],"metadata":{"id":"cWvYGMU-UNLK","executionInfo":{"status":"ok","timestamp":1683876308168,"user_tz":-180,"elapsed":371,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["mobilenet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_6IHhPoUP4b","executionInfo":{"status":"ok","timestamp":1683876311490,"user_tz":-180,"elapsed":372,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"8cdcff1d-affa-412a-cca5-08b3e66e50f2"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MobileNetV3(\n","  (features): Sequential(\n","    (0): Conv2dNormActivation(\n","      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","      (2): Hardswish()\n","    )\n","    (1): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n","          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n","          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n","          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n","          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n","          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (block): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (avgpool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n","          (activation): ReLU()\n","          (scale_activation): Hardsigmoid()\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (12): Conv2dNormActivation(\n","      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","      (2): Hardswish()\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (classifier): Sequential(\n","    (0): Linear(in_features=576, out_features=1024, bias=True)\n","    (1): Hardswish()\n","    (2): Dropout(p=0.2, inplace=True)\n","    (3): Linear(in_features=1024, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["num_classes = 10"],"metadata":{"id":"0iusNGAMUVea","executionInfo":{"status":"ok","timestamp":1683876326451,"user_tz":-180,"elapsed":2,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Change the final layer of mobilenet Model for Transfer Learning\n","fc_inputs = mobilenet.classifier[0].in_features\n","\n","mobilenet.classifier = nn.Sequential(\n","    nn.Linear(fc_inputs, 256),\n","    nn.Hardswish(),\n","    nn.Dropout(0.2),\n","    nn.Linear(256, num_classes), # Since 10 possible outputs\n","    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",")\n","\n","# Convert model to be used on GPU\n","mobilenet = mobilenet.to(device)"],"metadata":{"id":"F8fVllDbUWYq","executionInfo":{"status":"ok","timestamp":1683876327830,"user_tz":-180,"elapsed":2,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Define Optimizer and Loss Function\n","loss_func = nn.NLLLoss()\n","optimizer = optim.Adam(mobilenet.parameters(),lr=1e-3)"],"metadata":{"id":"23lHTGy3UaP9","executionInfo":{"status":"ok","timestamp":1683876370687,"user_tz":-180,"elapsed":2,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def train_and_validate_1(model, loss_criterion, optimizer, epochs=3):\n","    \n","    start = time.time()\n","    history = []\n","    best_loss = 100000.0\n","    best_epoch = None\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        #correct_5=0.0\n","        correct=0\n","        c=0\n","        total=0\n","        \n","        for i, (inputs, labels) in enumerate(train_data_loader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","#             print(\"Batch number: {:03d}, Valid: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","        \n","        #Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(valid_data_loader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","                \n","#                 # Compute top-5 accuracy\n","#                 total+=labels.size(0)\n","#                 c+=(predictions==labels).sum().item()\n","#                 pred=outputs.topk(4,largest=True,sorted=True)[0]\n","#                 lab=labels.view(labels.size(0),-1).expand_as(pred)\n","#                 correct=pred.eq(lab).float()\n","#                 correct_5+=correct[:,:4].sum()\n","                \n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","        \n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            best_epoch = epoch\n","\n","        #Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_data_size \n","        avg_train_acc = train_acc/train_data_size\n","\n","        # Find average training loss and training accuracy\n","        avg_valid_loss = valid_loss/valid_data_size \n","        avg_valid_acc = valid_acc/valid_data_size\n","\n","        history.append([avg_train_loss, avg_train_acc, avg_valid_loss, avg_valid_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.4f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n","#         print('Top 5 error:%2.2f' % (1-correct_5/total))        \n","#         # Save if the model has best accuracy till now\n","#         torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n","            \n","    return model, history, best_epoch"],"metadata":{"id":"3zVpjvgQxull","executionInfo":{"status":"ok","timestamp":1683877055845,"user_tz":-180,"elapsed":375,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Train the model for num epochs:\n","num_epochs = 5\n","trained_model, history, best_epoch = train_and_validate_1(mobilenet, loss_func, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZStR0hSsxun4","executionInfo":{"status":"ok","timestamp":1683878712930,"user_tz":-180,"elapsed":750907,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"6d07c91a-70b5-4c6a-bfd1-cccf62f1e1ed"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/5\n","Epoch : 000, Training: Loss - 0.7102, Accuracy - 75.1960%, \n","\t\tValidation : Loss - 0.5939, Accuracy - 79.0800%, Time: 152.0817s\n","Epoch: 2/5\n","Epoch : 001, Training: Loss - 0.6717, Accuracy - 76.5800%, \n","\t\tValidation : Loss - 0.5898, Accuracy - 79.6100%, Time: 150.2612s\n","Epoch: 3/5\n","Epoch : 002, Training: Loss - 0.6516, Accuracy - 77.3760%, \n","\t\tValidation : Loss - 0.6223, Accuracy - 78.3900%, Time: 149.9488s\n","Epoch: 4/5\n","Epoch : 003, Training: Loss - 0.6446, Accuracy - 77.5220%, \n","\t\tValidation : Loss - 0.5346, Accuracy - 81.6100%, Time: 149.0214s\n","Epoch: 5/5\n","Epoch : 004, Training: Loss - 0.6318, Accuracy - 78.0500%, \n","\t\tValidation : Loss - 0.5329, Accuracy - 81.3600%, Time: 149.4360s\n"]}]},{"cell_type":"code","source":["class Cifar10SearchDataset(torchvision.datasets.CIFAR10):\n","    def __init__(self, root=\"~/data/cifar10\", train=True, download=True, transform=None):\n","        super().__init__(root=root, train=train, download=download, transform=transform)\n","\n","    def __getitem__(self, index):\n","        image, label = self.data[index], self.targets[index]\n","\n","        if self.transform is not None:\n","            transformed = self.transform(image=image)\n","            image = transformed[\"image\"]\n","\n","        return image, label"],"metadata":{"id":"RI0vRjNTVQIa","executionInfo":{"status":"ok","timestamp":1683878774124,"user_tz":-180,"elapsed":451,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["train_transform = A.Compose([\n","        A.RandomResizedCrop(224,224),\n","        A.HorizontalFlip(p=0.5),\n","        A.RandomGamma(gamma_limit=(80, 120), eps=None, always_apply=False, p=0.5),\n","        A.RandomBrightnessContrast (p=0.5),\n","        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n","        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n","        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n","        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ])\n","\n","\n","\n","val_transform = A.Compose([\n","        A.Resize(256,256),\n","        A.CenterCrop(224,224),\n","        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ])"],"metadata":{"id":"UW863O-puo01","executionInfo":{"status":"ok","timestamp":1683878776742,"user_tz":-180,"elapsed":387,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["cifar_train = Cifar10SearchDataset(\n","root=\"./cifar\", train=True, download=True, transform=train_transform\n",")\n","\n","cifar_val = Cifar10SearchDataset(\n","root=\"./cifar\", train=False, download=True, transform=val_transform\n",")\n","\n","train_dataloader = torch.utils.data.DataLoader(\n","cifar_train, batch_size=32, shuffle=True, num_workers=4\n",")\n","\n","val_dataloader = torch.utils.data.DataLoader(\n","cifar_val, batch_size=32, shuffle=False, num_workers=4\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QR3CVuhXuzrs","executionInfo":{"status":"ok","timestamp":1683878786596,"user_tz":-180,"elapsed":1752,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"103acd8c-ff20-4eaf-c58d-06adcd6006c2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["train_num = len(cifar_train)\n","val_num=len(cifar_val)"],"metadata":{"id":"dYR0AzEiW-Bz","executionInfo":{"status":"ok","timestamp":1683878790471,"user_tz":-180,"elapsed":383,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["print(\"using {} images for training, {} images for  validation.\".format(train_num, val_num))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BcrWVUQkuzub","executionInfo":{"status":"ok","timestamp":1683878792582,"user_tz":-180,"elapsed":500,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"bea0256c-7198-43cf-a990-c216830ab0a9"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["using 50000 images for training, 10000 images for  validation.\n"]}]},{"cell_type":"code","source":["def train_and_validate(model, loss_criterion, optimizer, epochs=3):\n","    \n","    start = time.time()\n","    history = []\n","    best_loss = 100000.0\n","    best_epoch = None\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        #correct_5=0.0\n","        correct=0\n","        c=0\n","        total=0\n","        \n","        for i, (inputs, labels) in enumerate(train_dataloader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","#             print(\"Batch number: {:03d}, Valid: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","        \n","        #Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(val_dataloader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","                \n","#                 # Compute top-5 accuracy\n","#                 total+=labels.size(0)\n","#                 c+=(predictions==labels).sum().item()\n","#                 pred=outputs.topk(4,largest=True,sorted=True)[0]\n","#                 lab=labels.view(labels.size(0),-1).expand_as(pred)\n","#                 correct=pred.eq(lab).float()\n","#                 correct_5+=correct[:,:4].sum()\n","                \n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","        \n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            best_epoch = epoch\n","\n","        #Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_num \n","        avg_train_acc = train_acc/train_num\n","\n","        # Find average training loss and training accuracy\n","        avg_valid_loss = valid_loss/val_num \n","        avg_valid_acc = valid_acc/val_num\n","\n","        history.append([avg_train_loss, avg_train_acc, avg_valid_loss, avg_valid_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.4f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n","#         print('Top 5 error:%2.2f' % (1-correct_5/total))        \n","#         # Save if the model has best accuracy till now\n","#         torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n","            \n","    return model, history, best_epoch"],"metadata":{"id":"C_muMnxyYJlL","executionInfo":{"status":"ok","timestamp":1683878797103,"user_tz":-180,"elapsed":370,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Train the model for num epochs:\n","num_epochs = 5\n","trained_model, history, best_epoch = train_and_validate(mobilenet, loss_func, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OO9rqoekYhOE","executionInfo":{"status":"ok","timestamp":1683879751248,"user_tz":-180,"elapsed":948325,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"f55aaa0f-6f5d-458c-8efd-19a0fba82a68"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/5\n","Epoch : 000, Training: Loss - 1.1379, Accuracy - 60.1640%, \n","\t\tValidation : Loss - 0.5645, Accuracy - 80.6000%, Time: 186.7543s\n","Epoch: 2/5\n","Epoch : 001, Training: Loss - 1.0925, Accuracy - 61.7760%, \n","\t\tValidation : Loss - 0.5694, Accuracy - 80.1100%, Time: 186.9936s\n","Epoch: 3/5\n","Epoch : 002, Training: Loss - 1.0786, Accuracy - 61.7580%, \n","\t\tValidation : Loss - 0.5602, Accuracy - 80.5200%, Time: 187.6432s\n","Epoch: 4/5\n","Epoch : 003, Training: Loss - 1.0711, Accuracy - 62.6140%, \n","\t\tValidation : Loss - 0.5714, Accuracy - 80.5200%, Time: 193.6202s\n","Epoch: 5/5\n","Epoch : 004, Training: Loss - 1.0625, Accuracy - 62.8140%, \n","\t\tValidation : Loss - 0.5681, Accuracy - 80.9300%, Time: 193.2489s\n"]}]},{"cell_type":"code","source":["!pip install -q -U albumentations\n","!echo "],"metadata":{"id":"mBsnFzjJSS9W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683714504753,"user_tz":-180,"elapsed":4582,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"de5a0d17-2922-4d4d-eeb2-bd49b613eb6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\n"]}]},{"cell_type":"code","source":["! pip install --upgrade tensorflow_datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmrAdLVmNO0o","executionInfo":{"status":"ok","timestamp":1683714545620,"user_tz":-180,"elapsed":5117,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"10719c9e-515c-4f68-84ad-c43148387a89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.9.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.4.0)\n","Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.3)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.22.4)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (3.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.27.1)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.13.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.65.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.1)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (5.12.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.5.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.15.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow_datasets) (1.16.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.59.0)\n"]}]},{"cell_type":"markdown","source":["Попробую Torchvision, а именно AutoAugment с baseline configuration.\n"],"metadata":{"id":"sdgGyZcXP-Mi"}},{"cell_type":"code","source":["from torchvision.transforms.autoaugment import AutoAugmentPolicy"],"metadata":{"id":"N9794kF1QZPJ","executionInfo":{"status":"ok","timestamp":1683876993073,"user_tz":-180,"elapsed":408,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Applying Transforms to the Data\n","image_transforms_t = { \n","    'train': transforms.Compose([\n","             transforms.Resize(size=256),\n","             transforms.CenterCrop(size=224),\n","             transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n","             transforms.ToTensor(),\n","             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","             std=[0.229, 0.224, 0.225]),\n","                \n","                \n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","}"],"metadata":{"id":"zyq8ncrjQyxL","executionInfo":{"status":"ok","timestamp":1683876994669,"user_tz":-180,"elapsed":2,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Load the Data\n","\n","# Set train and valid directory paths\n","\n","dataset = 'cifar10'\n","\n","train_directory = os.path.join(dataset, 'train')\n","valid_directory = os.path.join(dataset, 'valid')\n","\n","# Batch size\n","bs = 32\n","\n","# Number of classes\n","num_classes = 10\n","\n","# Load Data from folders\n","data = {\n","    'train': torchvision.datasets.CIFAR10(root=\"./cifar\", train=True, download=True, transform=image_transforms_t['train']),\n","    'valid': torchvision.datasets.CIFAR10(root=\"./cifar\", train=False, download=True, transform=image_transforms_t['valid']),\n","}\n","\n","#Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n","idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n","print(idx_to_class)\n","\n","# Size of Data, to be used for calculating Average Loss and Accuracy\n","train_data_size = len(data['train'])\n","valid_data_size = len(data['valid'])\n","\n","# Create iterators for the Data loaded using DataLoader module\n","train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=True)\n","valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zu3ktS2hQyzt","executionInfo":{"status":"ok","timestamp":1683877016260,"user_tz":-180,"elapsed":10118,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"54d49e5f-823b-432e-9d06-85275647e79c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:05<00:00, 29677308.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar/cifar-10-python.tar.gz to ./cifar\n","Files already downloaded and verified\n","{0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n"]}]},{"cell_type":"code","source":["# Train the model for num epochs:\n","num_epochs = 5\n","trained_model, history, best_epoch = train_and_validate_1(mobilenet, loss_func, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhaSqe34QxmW","executionInfo":{"status":"ok","timestamp":1683877881023,"user_tz":-180,"elapsed":819876,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"360aa338-cf87-43c6-e54e-ec6e0fc1c798"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/5\n","Epoch : 000, Training: Loss - 0.9507, Accuracy - 66.8540%, \n","\t\tValidation : Loss - 0.5201, Accuracy - 81.6700%, Time: 169.0958s\n","Epoch: 2/5\n","Epoch : 001, Training: Loss - 0.7995, Accuracy - 71.7980%, \n","\t\tValidation : Loss - 0.4471, Accuracy - 84.3500%, Time: 163.0915s\n","Epoch: 3/5\n","Epoch : 002, Training: Loss - 0.7624, Accuracy - 73.1560%, \n","\t\tValidation : Loss - 0.4342, Accuracy - 84.5900%, Time: 164.0158s\n","Epoch: 4/5\n","Epoch : 003, Training: Loss - 0.7373, Accuracy - 74.1300%, \n","\t\tValidation : Loss - 0.3981, Accuracy - 86.2400%, Time: 161.6811s\n","Epoch: 5/5\n","Epoch : 004, Training: Loss - 0.7191, Accuracy - 74.8480%, \n","\t\tValidation : Loss - 0.4097, Accuracy - 85.8000%, Time: 161.7454s\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.DataFrame({'models': ['baseline transforms', 'albumentations', 'torchvision'], \n","                   'accuracy, %': [81.36, 80.93 , 85.8] \n","                   })"],"metadata":{"id":"2PX2CPsMQxpb","executionInfo":{"status":"ok","timestamp":1683879793387,"user_tz":-180,"elapsed":374,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"H-X6fG0NQxrq","executionInfo":{"status":"ok","timestamp":1683879795326,"user_tz":-180,"elapsed":5,"user":{"displayName":"Сергей Жуков","userId":"08674552531602215998"}},"outputId":"4b54f867-14cf-4ff3-854b-0c9e5b5ff67c"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                models  accuracy, %\n","0  baseline transforms        81.36\n","1       albumentations        80.93\n","2          torchvision        85.80"],"text/html":["\n","  <div id=\"df-c602aaf5-3ba3-460d-8f1d-1a2c2a76f1f5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>models</th>\n","      <th>accuracy, %</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>baseline transforms</td>\n","      <td>81.36</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>albumentations</td>\n","      <td>80.93</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>torchvision</td>\n","      <td>85.80</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c602aaf5-3ba3-460d-8f1d-1a2c2a76f1f5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c602aaf5-3ba3-460d-8f1d-1a2c2a76f1f5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c602aaf5-3ba3-460d-8f1d-1a2c2a76f1f5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":32}]}]}
